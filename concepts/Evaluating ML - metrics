# Learning about metrics to evaluate the Machine Learning algorithms performance.

Classification accuracy is the most used metric for measure the performance of our model, however it's not enough to truly judge our model.


# Classification metrics

    - Classification accuracy :
a.k.a accuracy. Ratio of (number of correct predictions) to (the total number of predictions made).
It works well only if there are equal number of samples belonging to each class.
Consider to have a train_set with much more data about A than B. If my test has the same shape it'll return a good accuracy, because it'll make good predictions on A. But if my test has more class B than A, probably the accuracy score will drop, my model doesn't learn too much about B.

    - Logarithmic Loss (Log Loss) :
works by penalising the false classifications. 
It works well for multi-class classification. 
The classifier must assign probability to each class for all the samples.

    - Confusion Matrix :
gives a matrix as output and describes the complete performance of the model.
matrix contains : True Positives and True Negatives (correct ones); 
                  False Positives and False Negatives (wrong ones).
Accuracy for the matrix is the (sum of the values lying across the “main diagonal”) / (the total number of predictions).
Confusion Matrix forms the basis for the other types of metrics.

    - Area Under Curve (AUC) :
It's used for binary classification problem. It tells how much a model is capable of distinguishing between classes.
AUC has a range of [0, 1]. The greater the value, the better is the performance of our model.
~True Positive Rate (Sensitivity) : TP/(FN+TP) - the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.
~True Negative Rate (Specificity) : TN/(FP+TN) - the proportion of negative data points that are correctly considered as negative, with respect to all negative data points.
~False Positive Rate : FP/(FP+TN) - the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.
TPR and FPR have values in the range [0, 1]. AUC is the area under the curve of plot FPR vs TPR at different points in [0, 1].

    - F1 Score :
The F1 score is a measure of a test’s accuracy. F1 Score is the harmonic mean between precision and recall. 
The range for F1 Score is [0, 1]. 1 = (perfect precision and recall).
It tells how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).
The greater the F1 Score, the better is the performance of our model.
~Precision : TP/(TP+FP). It's (the number of correct positive results) / (the number of positive results predicted). 
attempts to answer “What proportion of positive identifications was actually correct?”. 
precision_score(y_true, y_pred, average=None)
~Recall : TP/(TP+FN). It's (the number of correct positive results) / (the number of all relevant samples (all samples that should have been identified as positive)). 
attempts to answer “What proportion of actual positives was identified correctly?”.
recall_score(y_true, y_pred, average=None)


# Regression metrics

    - R-Squared :
R Squared is a measurement that represents the proportion of variance for the dependent variable that is explained by the variance in the independent variables. In other words, it tells how much a dependent variable varies relative to the independet variables.
If R² is 0.8, it means 80% of the variation in the output variable is explained by the input variables. If the R² is 1.0 or 100%, that means that all movements of the dependent variable can be entirely explained by the movements of the independent variables.
- Problem: it will either stay the same or increase with the addition of more independent variables, even if they don't have any relationship with the output variable.
    
    - Adjusted R-Squared :
Every additional independent variable added to a model always increases the R² value — therefore, a model with several independent variables may seem to be a better fit even if it isn’t. The adjusted R² compensates for each additional independent variable and only increases if each given variable improves the model above what is possible by probability. It penalizes you for adding variables that don't improve your existing model. 
If you are building Linear regression on multiple variables, it's always suggested that you use Adjusted R² to judge the goodness of the model. In case you only have one input variable, R² and Adjusted R² would be exactly the same.

    - Mean Absolute Error (MAE) :
it's the average of the difference between the original values and the predicted values, without considering their direction (absolute value*). It gives the measure of how far the predictions are from the actual output.
The greater MAE, the worse is the performance of our model. It's negatively-oriented scores, which means lower values are better.
*If the absolute value is not taken (the signs of the errors are not removed), the average error becomes the Mean Bias Error (MBE).

    - Mean Squared Error (MSE) :
is a measure of how close a fitted line is to data points. The smaller Mean Squared Error, the closer fit is to the data.
it's similar to MAE, the only difference being that MSE takes the average of the square differences between the original values and the predicted values.
The greater the MSE, the worse is the performance of our model.
As, we take square of the error, the effect of larger errors become more pronounced than smaller error, hence the model can now focus more on the larger errors.
this should be used over the MAE when you want to minimize large errors. 

    - Root Mean Squared Error (RMSE) :
It's negatively-oriented scores, which means lower values are better.
The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences.
It's the square root of the average of squared errors (MSE). The effect of each error on RMSE is proportional to the size of squared error, thus larger errors have a disproportionately large effect on RMSE. RMSE is sensitive to outliers.
For an unbiased estimator, the RMSE is the square root of the variance (standard deviation).

# References: 
https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234
https://towardsdatascience.com/how-to-evaluate-your-machine-learning-models-with-python-code-5f8d2d8d945b
